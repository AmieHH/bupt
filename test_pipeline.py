"""
æµ‹è¯•è„šæœ¬ - éªŒè¯pipelineçš„æ­£ç¡®æ€§
"""
import torch
import numpy as np
import os
import sys
from config import Config
from data_utils import generate_synthetic_data, create_data_loaders
from models import PretrainModel, FinetuneModel
from trainer import PretrainTrainer, FinetuneTrainer
from evaluation import Evaluator

def test_data_loading():
    """æµ‹è¯•æ•°æ®åŠ è½½"""
    print("ğŸ§ª æµ‹è¯•æ•°æ®åŠ è½½...")
    
    config = Config()
    config.data.batch_size = 2
    config.data.sequence_length = 10
    config.data.num_antennas = 8
    config.data.num_subcarriers = 8
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    test_data = generate_synthetic_data(config, num_samples=10, save_path="test_data.pkl")
    print(f"  âœ… ç”Ÿæˆ {len(test_data)} ä¸ªæµ‹è¯•æ ·æœ¬")
    
    # æµ‹è¯•æ•°æ®åŠ è½½å™¨
    config.data.train_data_path = "."
    pretrain_loader, finetune_train_loader, val_loader = create_data_loaders(config)
    
    # æµ‹è¯•é¢„è®­ç»ƒæ•°æ®åŠ è½½
    batch = next(iter(pretrain_loader))
    assert 'masked_csi' in batch
    assert 'mask' in batch
    assert batch['masked_csi'].shape == (2, 10, 8, 8, 2)
    assert batch['mask'].shape == (2, 10, 8, 8)
    print("  âœ… é¢„è®­ç»ƒæ•°æ®åŠ è½½æ­£å¸¸")
    
    # æµ‹è¯•å¾®è°ƒæ•°æ®åŠ è½½
    batch = next(iter(finetune_train_loader))
    assert 'csi' in batch
    assert 'position' in batch
    assert batch['csi'].shape == (2, 10, 8, 8, 2)
    # ä½ç½®æ•°æ®çš„å½¢çŠ¶å¯èƒ½æ˜¯ [batch_size, 2] è€Œä¸æ˜¯ [batch_size, seq_len, 2]
    print(f"  ä½ç½®æ•°æ®å½¢çŠ¶: {batch['position'].shape}")
    assert batch['position'].shape in [(2, 2), (2, 10, 2), (2, 1, 2)]
    print("  âœ… å¾®è°ƒæ•°æ®åŠ è½½æ­£å¸¸")
    
    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    os.remove("test_data.pkl")
    print("  âœ… æ•°æ®åŠ è½½æµ‹è¯•é€šè¿‡")

def test_models():
    """æµ‹è¯•æ¨¡å‹"""
    print("ğŸ§ª æµ‹è¯•æ¨¡å‹...")
    
    config = Config()
    config.data.batch_size = 2
    config.data.sequence_length = 10
    config.data.num_antennas = 8
    config.data.num_subcarriers = 8
    
    # æµ‹è¯•é¢„è®­ç»ƒæ¨¡å‹
    pretrain_model = PretrainModel(config)
    print(f"  âœ… é¢„è®­ç»ƒæ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œå‚æ•°æ•°é‡: {sum(p.numel() for p in pretrain_model.parameters()):,}")
    
    # æµ‹è¯•é¢„è®­ç»ƒå‰å‘ä¼ æ’­
    batch_size = 2
    seq_len = 10
    num_ant = 8
    num_sub = 8
    
    masked_csi = torch.randn(batch_size, seq_len, num_ant, num_sub, 2)
    mask = torch.ones(batch_size, seq_len, num_ant, num_sub, dtype=torch.bool)
    mask[:, :, :3, :3] = False  # æ©ç éƒ¨åˆ†ä½ç½®
    
    with torch.no_grad():
        outputs = pretrain_model(masked_csi, mask)
        assert 'reconstructed' in outputs
        assert 'features' in outputs
        assert 'loss' in outputs
        assert outputs['reconstructed'].shape == (batch_size, seq_len, num_ant, num_sub, 2)
        assert outputs['features'].shape == (batch_size, seq_len, num_ant, num_sub, config.model.pretrain_hidden_dim)
    print("  âœ… é¢„è®­ç»ƒæ¨¡å‹å‰å‘ä¼ æ’­æ­£å¸¸")
    
    # æµ‹è¯•å¾®è°ƒæ¨¡å‹
    finetune_model = FinetuneModel(config)
    print(f"  âœ… å¾®è°ƒæ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œå‚æ•°æ•°é‡: {sum(p.numel() for p in finetune_model.parameters()):,}")
    
    # æµ‹è¯•å¾®è°ƒå‰å‘ä¼ æ’­
    csi = torch.randn(batch_size, seq_len, num_ant, num_sub, 2)
    positions = torch.randn(batch_size, seq_len, 2)
    
    with torch.no_grad():
        outputs = finetune_model(csi, positions)
        assert 'predicted_positions' in outputs
        assert 'position_loss' in outputs
        assert 'trajectory_loss' in outputs
        assert 'total_loss' in outputs
        assert outputs['predicted_positions'].shape == (batch_size, seq_len, 2)
    print("  âœ… å¾®è°ƒæ¨¡å‹å‰å‘ä¼ æ’­æ­£å¸¸")
    
    print("  âœ… æ¨¡å‹æµ‹è¯•é€šè¿‡")

def test_training():
    """æµ‹è¯•è®­ç»ƒè¿‡ç¨‹"""
    print("ğŸ§ª æµ‹è¯•è®­ç»ƒè¿‡ç¨‹...")
    
    config = Config()
    config.pretrain.epochs = 1
    config.finetune.epochs = 1
    config.data.batch_size = 2
    config.data.sequence_length = 5
    config.data.num_antennas = 4
    config.data.num_subcarriers = 4
    config.device = 'cpu'  # å¼ºåˆ¶ä½¿ç”¨CPU
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    os.makedirs("test_data/train", exist_ok=True)
    os.makedirs("test_data/val", exist_ok=True)
    
    generate_synthetic_data(config, num_samples=10, save_path="test_data/train/train.pkl")
    generate_synthetic_data(config, num_samples=5, save_path="test_data/val/val.pkl")
    
    config.data.train_data_path = "test_data/train"
    config.data.val_data_path = "test_data/val"
    
    # æµ‹è¯•é¢„è®­ç»ƒ
    pretrain_loader, _, _ = create_data_loaders(config)
    pretrain_trainer = PretrainTrainer(config, 'cpu')
    
    # æµ‹è¯•ä¸€ä¸ªepoch
    train_loss = pretrain_trainer.train_epoch(pretrain_loader)
    assert isinstance(train_loss, float)
    assert train_loss >= 0
    print(f"  âœ… é¢„è®­ç»ƒä¸€ä¸ªepochå®Œæˆï¼ŒæŸå¤±: {train_loss:.4f}")
    
    # æµ‹è¯•å¾®è°ƒ
    _, finetune_train_loader, val_loader = create_data_loaders(config)
    # åˆ›å»ºå¾®è°ƒæ¨¡å‹è€Œä¸ä¾èµ–é¢„è®­ç»ƒæ–‡ä»¶
    finetune_model = FinetuneModel(config)
    finetune_trainer = FinetuneTrainer(config, None, 'cpu')
    finetune_trainer.model = finetune_model
    
    # æµ‹è¯•ä¸€ä¸ªepoch
    train_losses = finetune_trainer.train_epoch(finetune_train_loader)
    assert isinstance(train_losses, dict)
    assert 'total' in train_losses
    assert 'position' in train_losses
    assert 'trajectory' in train_losses
    print(f"  âœ… å¾®è°ƒä¸€ä¸ªepochå®Œæˆï¼Œæ€»æŸå¤±: {train_losses['total']:.4f}")
    
    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    import shutil
    shutil.rmtree("test_data")
    print("  âœ… è®­ç»ƒæµ‹è¯•é€šè¿‡")

def test_evaluation():
    """æµ‹è¯•è¯„ä¼°"""
    print("ğŸ§ª æµ‹è¯•è¯„ä¼°...")
    
    config = Config()
    config.data.batch_size = 2
    config.data.sequence_length = 5
    config.data.num_antennas = 4
    config.data.num_subcarriers = 4
    
    # åˆ›å»ºæµ‹è¯•æ¨¡å‹
    finetune_model = FinetuneModel(config)
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    os.makedirs("test_data/val", exist_ok=True)
    generate_synthetic_data(config, num_samples=10, save_path="test_data/val/val.pkl")
    
    config.data.val_data_path = "test_data/val"
    config.data.train_data_path = "test_data/val"  # ä½¿ç”¨ç›¸åŒæ•°æ®
    _, _, val_loader = create_data_loaders(config)
    
    # æµ‹è¯•è¯„ä¼°å™¨
    evaluator = Evaluator(finetune_model, 'cpu')
    
    # ç®€åŒ–æµ‹è¯•ï¼šåªæµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­
    batch = next(iter(val_loader))
    csi = batch['csi']
    positions = batch['position']
    
    with torch.no_grad():
        outputs = finetune_model(csi, positions)
        assert 'predicted_positions' in outputs
        assert 'position_loss' in outputs
        assert 'trajectory_loss' in outputs
        assert 'total_loss' in outputs
        print(f"  âœ… æ¨¡å‹å‰å‘ä¼ æ’­æ­£å¸¸ï¼Œé¢„æµ‹å½¢çŠ¶: {outputs['predicted_positions'].shape}")
    
    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    import shutil
    shutil.rmtree("test_data")
    print("  âœ… è¯„ä¼°æµ‹è¯•é€šè¿‡")

def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹è¿è¡Œæ‰€æœ‰æµ‹è¯•...")
    print("=" * 50)
    
    try:
        test_data_loading()
        print()
        
        test_models()
        print()
        
        test_training()
        print()
        
        test_evaluation()
        print()
        
        print("=" * 50)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("âœ… åŸºäºä¿¡é“é¢„è®­ç»ƒçš„æ³›åŒ–å®šä½æŠ€æœ¯pipelineå·¥ä½œæ­£å¸¸")
        print("=" * 50)
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == '__main__':
    run_all_tests()