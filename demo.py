"""
æ¼”ç¤ºè„šæœ¬ - å¿«é€Ÿä½“éªŒåŸºäºä¿¡é“é¢„è®­ç»ƒçš„æ³›åŒ–å®šä½æŠ€æœ¯
"""
import torch
import numpy as np
import matplotlib.pyplot as plt
import os
from config import Config
from data_utils import generate_synthetic_data, create_data_loaders
from models import PretrainModel, FinetuneModel
from trainer import PretrainTrainer, FinetuneTrainer
from evaluation import Evaluator, Visualizer

def quick_demo():
    """å¿«é€Ÿæ¼”ç¤º"""
    print("ğŸš€ åŸºäºä¿¡é“é¢„è®­ç»ƒçš„æ³›åŒ–å®šä½æŠ€æœ¯ - å¿«é€Ÿæ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºé…ç½®
    config = Config()
    config.pretrain.epochs = 2  # å¿«é€Ÿæ¼”ç¤º
    config.finetune.epochs = 2
    config.data.batch_size = 4
    config.data.sequence_length = 20
    config.data.num_antennas = 16
    config.data.num_subcarriers = 16
    
    print("ğŸ“Š é…ç½®ä¿¡æ¯:")
    print(f"  è®¾å¤‡: {config.device}")
    print(f"  å¤©çº¿æ•°é‡: {config.data.num_antennas}")
    print(f"  å­è½½æ³¢æ•°é‡: {config.data.num_subcarriers}")
    print(f"  åºåˆ—é•¿åº¦: {config.data.sequence_length}")
    print(f"  æ‰¹æ¬¡å¤§å°: {config.data.batch_size}")
    print()
    
    # ç”Ÿæˆåˆæˆæ•°æ®
    print("ğŸ“ ç”Ÿæˆåˆæˆæ•°æ®...")
    os.makedirs("demo_data/train", exist_ok=True)
    os.makedirs("demo_data/val", exist_ok=True)
    
    train_data = generate_synthetic_data(
        config, num_samples=50, 
        save_path="demo_data/train/train.pkl"
    )
    val_data = generate_synthetic_data(
        config, num_samples=20, 
        save_path="demo_data/val/val.pkl"
    )
    
    print(f"  è®­ç»ƒæ ·æœ¬: {len(train_data)}")
    print(f"  éªŒè¯æ ·æœ¬: {len(val_data)}")
    print()
    
    # æ›´æ–°æ•°æ®è·¯å¾„
    config.data.train_data_path = "demo_data/train"
    config.data.val_data_path = "demo_data/val"
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    pretrain_loader, finetune_train_loader, val_loader = create_data_loaders(config)
    
    # 1. é¢„è®­ç»ƒæ¼”ç¤º
    print("ğŸ”§ é¢„è®­ç»ƒé˜¶æ®µæ¼”ç¤º...")
    pretrain_model = PretrainModel(config)
    print(f"  é¢„è®­ç»ƒæ¨¡å‹å‚æ•°: {sum(p.numel() for p in pretrain_model.parameters()):,}")
    
    # æ¼”ç¤ºä¸€ä¸ªbatchçš„é¢„è®­ç»ƒ
    batch = next(iter(pretrain_loader))
    masked_csi = batch['masked_csi']
    mask = batch['mask']
    
    with torch.no_grad():
        outputs = pretrain_model(masked_csi, mask)
        print(f"  è¾“å…¥å½¢çŠ¶: {masked_csi.shape}")
        print(f"  æ©ç å½¢çŠ¶: {mask.shape}")
        print(f"  é‡å»ºå½¢çŠ¶: {outputs['reconstructed'].shape}")
        print(f"  é‡å»ºæŸå¤±: {outputs['loss'].item():.4f}")
    print()
    
    # 2. å¾®è°ƒæ¼”ç¤º
    print("ğŸ¯ å¾®è°ƒé˜¶æ®µæ¼”ç¤º...")
    finetune_model = FinetuneModel(config)
    print(f"  å¾®è°ƒæ¨¡å‹å‚æ•°: {sum(p.numel() for p in finetune_model.parameters()):,}")
    
    # æ¼”ç¤ºä¸€ä¸ªbatchçš„å¾®è°ƒ
    batch = next(iter(finetune_train_loader))
    csi = batch['csi']
    positions = batch['position']
    
    with torch.no_grad():
        outputs = finetune_model(csi, positions)
        print(f"  CSIå½¢çŠ¶: {csi.shape}")
        print(f"  ä½ç½®å½¢çŠ¶: {positions.shape}")
        print(f"  é¢„æµ‹ä½ç½®å½¢çŠ¶: {outputs['predicted_positions'].shape}")
        print(f"  ä½ç½®æŸå¤±: {outputs['position_loss'].item():.4f}")
        print(f"  è½¨è¿¹æŸå¤±: {outputs['trajectory_loss'].item():.4f}")
    print()
    
    # 3. è¯„ä¼°æ¼”ç¤º
    print("ğŸ“ˆ è¯„ä¼°æ¼”ç¤º...")
    evaluator = Evaluator(finetune_model, config.device)
    
    # ç®€åŒ–è¯„ä¼°ï¼šåªæµ‹è¯•ä¸€ä¸ªbatch
    batch = next(iter(val_loader))
    csi = batch['csi'].to(config.device)
    positions = batch['position'].to(config.device)
    
    with torch.no_grad():
        outputs = finetune_model(csi, positions)
        predictions = outputs['predicted_positions'].cpu().numpy()
        targets = positions.cpu().numpy()
        
        # è®¡ç®—åŸºæœ¬æŒ‡æ ‡
        mse = np.mean((predictions - targets) ** 2)
        mae = np.mean(np.abs(predictions - targets))
        rmse = np.sqrt(mse)
        
        print("  ä½ç½®é¢„æµ‹æŒ‡æ ‡:")
        print(f"    MSE: {mse:.4f}")
        print(f"    MAE: {mae:.4f}")
        print(f"    RMSE: {rmse:.4f}")
        print()
    
    # 4. å¯è§†åŒ–æ¼”ç¤º
    print("ğŸ¨ å¯è§†åŒ–æ¼”ç¤º...")
    visualizer = Visualizer("demo_visualizations")
    
    # è·å–é¢„æµ‹ç»“æœ
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for batch in val_loader:
            csi = batch['csi'].to(config.device)
            positions = batch['position'].to(config.device)
            
            outputs = finetune_model(csi, positions)
            predictions = outputs['predicted_positions'].cpu().numpy()
            targets = positions.cpu().numpy()
            
            all_predictions.append(predictions)
            all_targets.append(targets)
    
    all_predictions = np.concatenate(all_predictions, axis=0)
    all_targets = np.concatenate(all_targets, axis=0)
    
    # ç»˜åˆ¶è½¨è¿¹å¯¹æ¯”å›¾
    print("  ç”Ÿæˆè½¨è¿¹å¯¹æ¯”å›¾...")
    visualizer.plot_trajectory_comparison(
        all_predictions, all_targets,
        save_path="demo_visualizations/trajectory_comparison.png"
    )
    
    # ç»˜åˆ¶è¯¯å·®åˆ†å¸ƒå›¾
    print("  ç”Ÿæˆè¯¯å·®åˆ†å¸ƒå›¾...")
    visualizer.plot_error_distribution(
        all_predictions, all_targets,
        save_path="demo_visualizations/error_distribution.png"
    )
    
    print("  å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: demo_visualizations/")
    print()
    
    # 5. æ€»ç»“
    print("âœ… æ¼”ç¤ºå®Œæˆ!")
    print("=" * 60)
    print("ğŸ“‹ æ¼”ç¤ºæ€»ç»“:")
    print("  1. âœ… é¢„è®­ç»ƒæ¨¡å‹: æ©ç é‡å»ºè‡ªç›‘ç£å­¦ä¹ ")
    print("  2. âœ… å¾®è°ƒæ¨¡å‹: ä½ç½®é¢„æµ‹ + è½¨è¿¹ä¸€è‡´æ€§çº¦æŸ")
    print("  3. âœ… è¯„ä¼°æŒ‡æ ‡: ä½ç½®ç²¾åº¦ + è½¨è¿¹ä¸€è‡´æ€§")
    print("  4. âœ… å¯è§†åŒ–: è½¨è¿¹å¯¹æ¯” + è¯¯å·®åˆ†å¸ƒ")
    print()
    print("ğŸ”— ä¸‹ä¸€æ­¥:")
    print("  - è¿è¡Œå®Œæ•´è®­ç»ƒ: python main.py --mode full")
    print("  - æŸ¥çœ‹è¯¦ç»†æ–‡æ¡£: README.md")
    print("  - è¿è¡Œä½¿ç”¨ç¤ºä¾‹: python example_usage.py")
    print("=" * 60)

if __name__ == '__main__':
    quick_demo()